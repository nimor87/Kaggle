{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooled GRU + GloVe in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is designed to replicate the methods presented in https://www.kaggle.com/yekenot/pooled-gru-fasttext in PyTorch. I have recently started learning Pytorch after i saw it mentioned in the fast.ai lectures. I could not clearly figure out some of the features which i will state as questions in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependent modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import time\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.legacy.nn as legacy\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Text Preprocessing\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "# Tokenizer\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer. Using SpaCy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP = spacy.load('en')\n",
    "\n",
    "def tokenizer(comment):\n",
    "    comment = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\!+\", \"!\", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    comment = re.sub(r\"\\?+\", \"?\", comment)\n",
    "    return [x.text for x in NLP.tokenizer(comment) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a torchtext data.Field defining how to treat the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = data.Field(\n",
    "        sequential=True,\n",
    "        fix_length=200,\n",
    "        tokenize=tokenizer,\n",
    "        pad_first=True,\n",
    "        lower=True\n",
    "    )\n",
    "LABEL = data.Field(\n",
    "        sequential=False,\n",
    "        use_vocab=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the text with tokens. Rename path to wherever you datasets are stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TabularDataset(\n",
    "        path='Dataset/train.csv', format='csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('id', None),\n",
    "            ('comment_text', COMMENT),\n",
    "            ('toxic', LABEL),\n",
    "            ('severe_toxic', LABEL),\n",
    "            ('obscene', LABEL),\n",
    "            ('threat', LABEL),\n",
    "            ('insult', LABEL),\n",
    "            ('identity_hate', LABEL),\n",
    "        ])\n",
    "test = data.TabularDataset(\n",
    "        path='Dataset/test.csv', format='csv', skip_header=True,\n",
    "        fields=[\n",
    "            ('id', None),\n",
    "            ('comment_text', COMMENT)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary with the 50000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT.build_vocab(\n",
    "        train, test,\n",
    "        max_size=50000,\n",
    "        min_freq=0,\n",
    "        vectors=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained GloVe word embeddings with 300 dimensions. I tried to use Fasttesxt but there was an error message regarding the dimensionality of some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT.vocab.load_vectors('glove.6B.300d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create batches for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.Iterator(train, batch_size=64, sort_within_batch=False, \n",
    "                           device=-1, sort_key=lambda x: len(x.comment_text), repeat=False, shuffle=False)\n",
    "test_iter = data.Iterator(test, batch_size=64, device=-1, sort=False, \n",
    "                          sort_within_batch=False, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not figure out exactly where the text gets actually one-hot-encoded. It seems as long as you don't create iterators the vocabulary does not get applied. I would like to know if it is possible to not create iterators as a mandatory step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(COMMENT.vocab)\n",
    "hidden_size = 80\n",
    "batch_size = 64\n",
    "embedding_size = 300\n",
    "label_size = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, label_size, batch_size):\n",
    "        super(ToxicCommentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.word_embeddings.weight.data.copy_(COMMENT.vocab.vectors)\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        self.spatialdropout = nn.Dropout2d(p=0.2)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size*2*2, label_size)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.hidden = self.init_hidden()\n",
    "        x = self.word_embeddings(x)\n",
    "        x = self.spatialdropout(x.transpose(1,0).contiguous())\n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "        maxpool, _ = torch.max(output, dim=0)\n",
    "        meanpool = torch.mean(output, dim=0)\n",
    "        concat = torch.cat((maxpool, meanpool), dim=1)\n",
    "        y_pred = self.linear(concat)\n",
    "        return y_pred\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(2, self.batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently SpatialDropout exists only in the legacy modules in PyTorch. However, if i am not mistaken Dropout2d is the equivalent as it zeroes out whole channels. I am still uncertain about this point, though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model, optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToxicCommentClassifier(embedding_size, hidden_size, vocab_size, label_size, batch_size)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_inputs_labels(batch):\n",
    "    x = batch.comment_text\n",
    "    y = torch.stack([\n",
    "        batch.toxic, batch.severe_toxic, \n",
    "        batch.obscene,\n",
    "        batch.threat, batch.insult, \n",
    "        batch.identity_hate\n",
    "    ], dim=1)\n",
    "    y = y.type(torch.FloatTensor)\n",
    "    x = x.transpose(1,0).contiguous()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [13:47<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.00104232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [13:51<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.000816634\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_iter):\n",
    "        inputs, labels = return_inputs_labels(batch)\n",
    "        model.batch_size = len(inputs.data)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.data[0]\n",
    "       \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    \n",
    "    \n",
    "    train_loss = train_loss / len(train)\n",
    "    \n",
    "    print ('Epoch: %d, Training Loss: %g' % ((i+1), train_loss))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2394/2394 [08:00<00:00,  4.98it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_list = []\n",
    "for batch in tqdm(test_iter):\n",
    "    x = batch.comment_text\n",
    "    inputs = x.transpose(1,0).contiguous()\n",
    "    model.batch_size = len(inputs.data)\n",
    "    predictions = model(inputs)\n",
    "    predictions = predictions.data.numpy()\n",
    "    predictions = 1 / (1 + np.exp(-predictions))\n",
    "    prediction_list.append(predictions)\n",
    "predictions = np.vstack(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"Dataset/test.csv\")\n",
    "for i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n",
    "    submission[col] = predictions[:, i]\n",
    "submission.drop(\"comment_text\", axis=1).to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the public leaderboard with this notebook is 0.9670. \n",
    "\n",
    "If i use no dropout i could get up to 0.9783 - which leads me too believe Dropout2d is not exactly doing what i think it is doing!?\n",
    "\n",
    "Also i find it really difficult to implement proper validation techniques as i am always only able to deal with the data in batches and i get lost somewhere in the for-loops.\n",
    "\n",
    "Again, i am very new to this and thought i would play around with PyTorch a little bit as most of the kernels are written with Keras. I think it is a great framework but the documentation is lacking at some specific points so i am glad for any suggestions on how to optimize my code. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
